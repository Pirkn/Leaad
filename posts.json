["\n            === POST 0 ===\n            Title: What are the most populous climates on Earth? [OC]\n            Score: 20 upvotes\n            Total Comments: 2\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mn65mt/\n            Content: No text\n\n            TOP COMMENTS:\n        1. Score: 1 | Comment: Data: kontur (world population database) abnd Beck et al. (2018) for the K\u00f6ppen climate zones\n\nData was processed in QGIS and the rest was done on adobe illustrator for the graphic design...\n\nK\u00f6ppen climate maps are great but what about human population in these climates? I tried to bring a visual answer to that question with the population estimates (population and population density) for each of the 30 types there are. \n\nHope you like the result and find it interesting!\n\nif you want to see more, here's my [website](https://www.perrinremonte.com/fraccueil-1) for geo / map nerds! :) (not smartphone friendly)\n2. Score: 1 | Comment: Was about to say that Aw figures as that's where humans evolved, until I saw the population density figures.\n\nSurprised Cfa doesn't score more highly than Dwa on population density. I guess that Cfa is weighed down by the Southern US whereas Dwa only really exists around Northern China/Manchuria.\n==============================", "\n            === POST 1 ===\n            Title: Visualization of pinball machines in the US's lower 48\n            Score: 31 upvotes\n            Total Comments: 1\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mn2c2o/\n            Content: No text\n\n            TOP COMMENTS:\n        1. Score: 1 | Comment: I was about to get on you for just another population density map but I was pleasantly surprised\n==============================", "\n            === POST 2 ===\n            Title: RNA code visualization\n            Score: 0 upvotes\n            Total Comments: 0\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mn0b5t/\n            Content: I am working on a mathematical model of the billions of years old RNA code. here is the visualization \n\n            TOP COMMENTS:\n        ==============================", "\n            === POST 3 ===\n            Title: [OC] Where the Class of 2021 Went: A Look at Post-Graduation Plans from a Long Island High School that I attended.\n            Score: 379 upvotes\n            Total Comments: 73\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mmv02f/\n            Content: Its a interactive map so when you hover over some of the dots it show how many people went to that specific college. It prints a individual dot no matter if its 1 or 10 people going to the same college. I'm just not sure if there's a good way to show that? Perhaps color coding but it would get confusing. I can prob make the html a viewable link if anyone is curious to see. This was just a quick stab while I continue to learn python.\n\n            TOP COMMENTS:\n        1. Score: 168 | Comment: Shout out to the one kid going to Cal Poly SLO.\n2. Score: 53 | Comment: Data source : the commencement paper I got when graduating with name and post secondary plans.  \ntools used : python pandas to read the csv file of the names and schools from the pamphlet which i scanned with my phone which I converted into a csv. then i used something called geopy and follium\n3. Score: 42 | Comment: You can scale the size of the circle to visualize number of people for each dot. Bigger circle = more people.\n==============================", "\n            === POST 4 ===\n            Title: [OC] Representational Alignment Index: How well each state's House delegation matches 2024 voter preferences (CORRECTED)\n            Score: 58 upvotes\n            Total Comments: 8\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mmuo6l/\n            Content: **CORRECTED VERSION - Thank you for the feedback!**\n\nThis is a corrected version of my previous RAI visualization. Special thanks to u/quitefondofdarkroast and u/Deto for their sharp observations that helped identify calculation errors in my original dataset. Their feedback on Texas and Ohio's scores led me to do a complete verification of all 50 states.\n\n**What was fixed:**\n\n* Recalculated all RAI scores from scratch using verified source data\n* Corrected House delegation counts (e.g., New York had 7 Republicans, not 11)\n* Double-checked calculations against multiple examples\n\n**Key findings remain the same:**\u00a0Single-representative states tend to show the highest misalignment due to winner-take-all effects, while larger states generally show better proportional representation.\n\nThe methodology is sound - it was my execution that needed improvement. This is exactly why peer review matters in data analysis!\n\n            TOP COMMENTS:\n        1. Score: 9 | Comment: This would work way better with actual data in the map clicks. All you provide is the final RAI. It fairly easy to provide the 2 key data points that make up that number - presidential percentage and house percentages.\n\nTexas, for example, was 56% Trump / 42% Harris. The congressional reps are 66% R / 33% D.\n2. Score: 4 | Comment: I wonder if total house votes vs house reps would be a better comparison or just different? Would be interesting to see if there were any difference using President vs House votes. I don't know that would signify, if anything, just interesting.\n3. Score: 1 | Comment: **Data Sources:**\n\n* 2024 Presidential Election Results: Federal Election Commission (https://www.fec.gov/documents/5644/2024presgeresults.pdf)\n* Current House of Representatives composition: Wikipedia List of Current U.S. Representatives and official House records\n\n**Tool:**\u00a0Datawrapper\n\n**Methodology:**\u00a0The Representational Alignment Index (RAI) measures the absolute difference between a state's presidential vote margin and its House delegation partisan composition. RAI = |Presidential Margin - House Margin|, where Presidential Margin = Trump% - Harris% and House Margin = Republican seat% - Democratic seat%. A score of 0 represents perfect alignment, while higher scores indicate greater misalignment between voter preferences and representation.\n\n**Key Findings:**\u00a0Single-representative states show the highest misalignment (Alaska 86.8, Wyoming 53.5) due to winner-take-all effects. Best aligned states include ~~Maine (3.1), New Hampshire (2.7), and Pennsylvania (4.4)~~. The analysis reveals geographic patterns where small states often cannot achieve proportional representation due to having only one House seat.\n\n**Limitations:**\u00a0Uses presidential vote as proxy for partisan preference rather than actual House vote totals. Assumes presidential voting reflects general partisan preference, which may not capture voters who prefer divided government or have different executive vs. legislative preferences.\n\n**Note:**\u00a0This is a corrected version addressing calculation errors identified by the community in my previous post.\n==============================", "\n            === POST 5 ===\n            Title: Year wise growth of Installed electricity capacity of India (in percentage)[OC]\n            Score: 0 upvotes\n            Total Comments: 11\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mmqmlq/\n            Content: Source: e Sankhyiki Portal (Energy Statistics of India)  \nTools used: Python  \nLibraries: Pandas, Matplotlib, FuncAnimation\n\n            TOP COMMENTS:\n        1. Score: 11 | Comment: What do you mean by \"steam\"? Steam isn't a power source. Are you saying geothermal? Coal?\n2. Score: 2 | Comment: **Question:**\u00a0Great work visualizing the growth...especially the breakdown by source. Can you share what\u2019s included under \u201cRES\u201d? Does that category cover only solar and wind, or does it also include biomass, small hydro, and waste-to-energy? Would be interesting to see those sub-sources split out in a future version.\n3. Score: 1 | Comment: Looks interesting. Any idea what happened in 2019?\n==============================", "\n            === POST 6 ===\n            Title: [OC] Obesity prevalence across Indian districts.\n            Score: 264 upvotes\n            Total Comments: 76\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mmp46y/\n            Content: No text\n\n            TOP COMMENTS:\n        1. Score: 213 | Comment: Seems effectively same as a socio-economic map or human development index map, basically. Surely isn\u2019t a coincidence.\n2. Score: 63 | Comment: Is it obesity percentage, or overweight and obesity percentage? Because the post title says one thing and the chart title says something else.\n3. Score: 41 | Comment: First you get the money.... Then you get the food... Then you get fat...\n==============================", "\n            === POST 7 ===\n            Title: LLM's play Prisoner's Dilemma: smaller models achieve higher rating [OC]\n            Score: 58 upvotes\n            Total Comments: 18\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mmmajk/\n            Content: source (data, methods, and info): [dilemma.critique-labs.ai](http://dilemma.critique-labs.ai)  \ntools used: Python  \n  \nI ran a benchmark where 100+ large language models played each other in a conversational formulation of the Prisoner\u2019s Dilemma  (100 matches per model, round-robin).  \n\nInterestingly, regardless of model series as they get larger they lose their tendency to defect (choose the option to save themselves at the cost of their counterpart) , and also subsequently perform worse. \n\n**Data & method:**\n\n* 100 games per model, \\~10k games total\n* Payoff matrix is the standard PD setup\n* Same prompt + sampling parameters for each model\n\n\n\n            TOP COMMENTS:\n        1. Score: 62 | Comment: Where is the key? Am I supposed to guess what the colors mean?  The labels are illegible.  They may be readable on a desktop, but on a mobile device (60% of reddit users), they are a blur.\n2. Score: 29 | Comment: Did models retain state between matches? If not, then there's no point in actually doing a round robin, just get a sample from each model to estimate defect/cooperate rate. That's enough to let you compute the expected scores.\n\nThe nature of the game means that the rating would be a function of the portion of cooperating peers, so it seems like ELO says more about the selection of the pool rather than general \"strength\" of a model. \n\nI'd be interested in seeing results for an iterated prisoners dilemma.\n\nI'm terms of the presentation itself, the \"clustered by variant\" isn't great since it's unclear how much data is being hidden. I wonder if a scatterplot of model size vs ELO / model size vs cooperation rate would be better. Points colored by model name.\n3. Score: 7 | Comment: > and also subsequently perform worse\n\nBy what definition of \"perform\"? LLMs are not designed to optimize short-term gains in thought experiments - they are designed to mimic what a *human* would say when given the same prompt. As models get better, they more accurately mimic what a human would say. Evidently, the humans in their training data would choose not to defect\n==============================", "\n            === POST 8 ===\n            Title: ISBN Visualization\n            Score: 26 upvotes\n            Total Comments: 0\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mmk0sa/\n            Content: No text\n\n            TOP COMMENTS:\n        ==============================", "\n            === POST 9 ===\n            Title: Visualizing Paul\u2019s Journeys Across the 1st Century Roman World [OC]\n            Score: 59 upvotes\n            Total Comments: 5\n            URL: https://www.reddit.com/r/dataisbeautiful/comments/1mmiq4w/\n            Content: No text\n\n            TOP COMMENTS:\n        1. Score: 7 | Comment: I created an interactive map overlaying Paul\u2019s 20,000km of journeys on a 1st century Roman Roads network - with modern vs. ancient cities and site photos. The base map utilizes the Digital Atlas of the Roman Empire (DARE), which was embedded into ArcGIS, with all four of Paul\u2019s journeys with every stop added. The Roman Roads map can also be switched to a modern map to compare the ancient vs. modern locations.\n\nThis is part of a personal project I am embarking on called Kingdoms Collide, where I plan to retrace every step of Paul\u2019s journeys across the ancient Roman Roads. Approximately 30% is complete, and I hope to finish it in the next couple years. I find it fascinating the relationship between the message what Paul brought versus that of the Roman Empire, where the world was never the same following those encounters. Hope you enjoy exploring through this map!\n\nSource: The Book of Acts and on-site research\nTool: ArcGis and the DARE Atlas\n2. Score: 6 | Comment: First I was like: Paul? Who is Paul and what is he doing in ancient Rome? \n\nUntil I realized you were writing about Saint Paul and of course during his travels he wasn't a saint yet.\n\nGreat work.\n==============================", "\n            === POST 10 ===\n            Title: Salary raise expectations\n            Score: 1 upvotes\n            Total Comments: 0\n            URL: https://www.reddit.com/r/businessintelligence/comments/1mmhaze/\n            Content: No text\n\n            TOP COMMENTS:\n        ==============================", "\n            === POST 11 ===\n            Title: Analysing your SEO Metadata\n            Score: 0 upvotes\n            Total Comments: 0\n            URL: https://www.reddit.com/r/businessintelligence/comments/1mme516/\n            Content: I have always had a passion to help fix the internet. After all it is a mix of structured and unstructured data. The problem,  a lack of accurate metadata to support on page content.\n\nTo help understand the root cause:\n\n# Beyond Keywords: Why Deterministic SEO Principles Eliminate Hallucination\n\nThe SEO landscape is experiencing a fundamental shift. Traditional keyword-based optimisation, rooted in probabilistic guesswork, is giving way to deterministic approaches that leverage structured metadata and schema markup. This evolution isn't just about better rankings\u2014it's about eliminating the \"hallucination\" that has plagued SEO for decades. Exacerbated by AI.\n\n## The Problem with Probabilistic SEO\n\nTraditional SEO operates on probability. We guess which keywords might work, estimate search volumes, and hope our content aligns with user intent. This approach creates several issues:\n\n- **Content-context disconnect**: Keywords often don't capture true user intent ( which is difficult to com\n\n            TOP COMMENTS:\n        ==============================", "\n            === POST 12 ===\n            Title: Does anyone use R Shiny at work ?\n            Score: 26 upvotes\n            Total Comments: 12\n            URL: https://www.reddit.com/r/businessintelligence/comments/1mm36ib/\n            Content: I know Python is widely used, but I recently tried this approach. Honestly, it blows everything out including powerBI and tableau if you know some coding. We had to analyze very large datasets \u2014 over a million rows and more than 100 variables for 29 different datasets, around 100GB data.   A key part of the task was identifying the events and timeframes that caused changes in the target variable relative to others. A lot of exploratory analysis had to done in the beginning, where the data had to be zoomed in very close. Plotly in shiny was very helpful along with JavaScript functions to customize the hover behavior\n\n\nUsing R, along with its powerful statistical capabilities, Shiny and Plotly packages, made the analysis significantly easier. I was able to use Plotly\u2019s event triggers to interactively subset the data and perform targeted analysis within the app itself. Data was queried from duckdb \n\nNo one in my company was aware of this approach before. After seeing it in action, and how\n\n            TOP COMMENTS:\n        1. Score: 12 | Comment: I used it years ago at work.  That's when I was department head, so got to choose.  These days, after I've switched jobs, leadership at most places will only consider software that's cool and popular, and if it has AI somewhere in the name of it.\n2. Score: 6 | Comment: shiny\u2019s a sleeper hit for exactly that reason\u2014if you can code, it wipes the floor with point and click tools for custom workflows  \nplus you own the logic instead of being boxed into whatever visual or calc options tableau/powerbi let you have\n\nbiggest wins: reproducibility, flexibility, and how fast you can go from concept to deploy  \ndownside is the learning curve for non coders, but in a team of analysts/engineers it\u2019s a game changer\n\nThe [NoFluffWisdom Newsletter](https://NoFluffWisdom.com/Subscribe) has some sharp takes on picking the right tools for impact and getting your org to adopt them worth a peek!\n3. Score: 3 | Comment: If my audience is more technical and the need is complex, I enjoy using R (Shiny, Markdown, etc.) and I think it's fit for the job. I'd never make a tool for widespread use with it, though. I'm not asking my execs or marketing team to use a shiny app.\n==============================", "\n            === POST 13 ===\n            Title: Quick thoughts on this data cleaning application?\n            Score: 0 upvotes\n            Total Comments: 0\n            URL: https://www.reddit.com/r/businessintelligence/comments/1mlxvhf/\n            Content: Hey everyone! I'm working on a project to combine an AI chatbot with comprehensive automated data cleaning. I was curious to get some feedback on this approach?\n\n* What are your thoughts on the design?\n* Do you think that there should be more emphasis on chatbot capabilities?\n* Other tools that do this way better (besides humans lol)\n\n            TOP COMMENTS:\n        ==============================", "\n            === POST 14 ===\n            Title: The dashboard is fine. The meeting is not. (honest verdict wanted)\n            Score: 8 upvotes\n            Total Comments: 23\n            URL: https://www.reddit.com/r/businessintelligence/comments/1ml4qdw/\n            Content: *(I've used ChatGPT a little just to make the context clear)*\n\nI hit this wall every week and I'm kinda over it. The dashboard is \"done\" (clean, tested, looks decent). Then Monday happens and I'm stuck doing the same loop:\n\n* Screenshots into PowerPoint\n* Rewrite the same plain-English bullets (\"north up 12%, APAC flat, churn weird in June\u2026\")\n* Answer \"what does this line mean?\" for the 7th time\n* Paste into Slack/email with a little context blob so it doesn't get misread\n\nIt's not analysis anymore, it's translating. Half my job title might as well be \"dashboard interpreter.\"\n\n# The Root Problem\n\nAt least for us: most folks don't speak dashboard. They want the so-what in their words, not mine. Plus everyone has their own definition for the same metric (marketing \"conversion\" \u2260 product \"conversion\" \u2260 sales \"conversion\"). Cue chaos.\n\n# My Idea\n\nSo\u2026 I've been noodling on a tiny layer that sits on top of the BI stuff we already use (Power BI + Tableau). Not a new BI tool, not another place\n\n            TOP COMMENTS:\n        1. Score: 37 | Comment: I mean, I think that\u2019s exactly where our org is headed with the new stand alone copilot interface. We probably won\u2019t focus on dashboards at all, just semantic layer so people can \u201cgoogle\u201d their data answers without using any brain cells. Will it be accurate? No clue. I\u2019ve been in data for 15 years. I\u2019m starting to think people don\u2019t actually give af about accuracy\n2. Score: 9 | Comment: I just assume either the dashboards are bad at telling the story or the stakeholders are idiots. I try to make my dashboards tell the story the way the stakeholders want it to be told. I add tabs that explain everything in detail and a tab for easily exporting tabular data into Excel if they need it.\n\nIt\u2019s very rare for me to need to explain everything. \n\nBut my experience is at a F100 company, it might be a little different at a smaller company.\n\nMy gut instinct is if a stakeholder is too stupid to read a few charts then they are likely too stupid for their role. They wont be around long and I will never do their job for them.\n3. Score: 4 | Comment: Your idea is the actual Microsoft product strategy.\u00a0\n==============================", "\n            === POST 15 ===\n            Title: When your team speaks 5 different data dialects\n            Score: 9 upvotes\n            Total Comments: 12\n            URL: https://www.reddit.com/r/businessintelligence/comments/1ml3ida/\n            Content: It's interesting how a single metric can have 5 different meanings for 5 different people. Last month, we discussed \"conversion rate\" in a cross-department review. Sales thought it meant leads-to-customers. Marketing thought it referred to ad clicks to signups. Product saw it as trial-to-paid. The data team? We had our own definition.\n\nThis led to 20 minutes of back-and-forth, with everyone saying, \"Wait, that's not what I meant.\"\n\nThis situation happens more often than I\u2019d like to admit. Each time, I wonder if our real problem isn\u2019t data access but the language we use around data. You can have the best dashboard, but if everyone reads it in their own way, you\u2019re just creating pretty graphs for confusion.\n\n\n\nWe\u2019ve tried:\n\n\\- Creating a glossary in Notion (but half the team ignores it)\n\n\\- Adding metric definitions on the dashboards themselves (some people still skip them)\n\n\\- Holding weekly \u201cdata office hours\u201d (where attendance is low)\n\n\n\nSometimes, I think the solution is less about t\n\n            TOP COMMENTS:\n        1. Score: 17 | Comment: You simply cannot use general colloquial phrases for measures. Everyone translates them however they see it rather than making sure their assumptions are correct. \n\nYou *have to* call it \"Lead to Customer Conversion Rate\",  \"Ad Click Conversion Rate\", etc. \n\n\"New Profit\" is a big one at my work. Everyone thinks of \"New\" as something different. Is it a customer that signed up this calendar year? Is it a customer that signed up sometime in the past 365 days? Is it a customer who signed up this month? The answer is yes, all of those are different types of \"new,\" so be clear on which type you're talking about in your measures and dashboards.\n2. Score: 6 | Comment: I can\u2019t even get my team to decide what day is the beginning of the week\n3. Score: 4 | Comment: Having a data dictionary that clearly defines each metric helps but is often just another piece of dead documentation if you don't have regular discussions with the teams your dashboards serve.\n\nA proactive approach by teamleads/management/product management is necessary to keep most of the teams on the same page and is unfortunately an ongoing challenge.\n\nThe key group to invest in are the official (and non-official!) decision makers in the organization. If you can get them to understand how they can use the data and dashboards and actually employ the data to drive decisions, my experience is the teams themselves invest their time to understand the metrics as well. Because that understanding now affects their day-to-day.\n\nI would advise against using terms like just \"conversion\" or \"conversion rate\" on its own in your dashboards though. You can't police language in the organization but you can make sure you don't add to the confusion. Make sure you specify the metrics like \"add-to-click conversion rate\".\n\nI try to add buttons that give users context for the dashboards or even specific visualisations where i explain the data with explicit text like \"what's this?!\" or \"what am I looking at?!\" on the button.\n\nBut the main thing is, explain the data, in person, over, and over, and over, and over again. No way around it I'm afraid...\n\nBest of luck!\n==============================", "\n            === POST 16 ===\n            Title: If you could automate ONE annoying step in your reporting workflow, what would it be?\n            Score: 0 upvotes\n            Total Comments: 8\n            URL: https://www.reddit.com/r/businessintelligence/comments/1ml01p1/\n            Content: Setting aside data quality for a second\u2014what's the one repetitive task in your reporting process you'd automate instantly if you could?\n\nPersonally, I'm stuck on manual narrative creation\u2014writing explanations that translate dashboards into actionable insights for execs.\n\nWould you trust a tool that auto-generated these narratives? What would it have to do (learn your internal KPIs, use company-specific language, etc.) to win your confidence?\n\n            TOP COMMENTS:\n        1. Score: 5 | Comment: Such a dumb thing but when I\u2019m making dashboards I hate that when I have a list of metrics making metric - latest month, metric - MoM%, metric - YoY% \n\nSoooo annoying that I have to do this for each metric\n2. Score: 4 | Comment: Working at a company that builds AI agents and workflows, the manual narrative creation problem you mentioned is exactly what kills most reporting processes and honestly, most business intelligence tools completely ignore the \"so what\" question that executives actually need answered.\n\nThe automation I'd kill for is dynamic variance analysis that automatically identifies why metrics changed and provides business context. Not just \"sales dropped 15%\" but \"sales dropped 15% primarily due to delayed enterprise deals in Q4, with SMB segment actually growing 8% despite seasonal trends.\"\n\nFor narrative generation to work, the tool would need deep understanding of your business model, seasonal patterns, competitive landscape, and internal terminology. Generic \"insights\" are useless because they lack the business context that makes analysis actionable.\n\nThe trust factor comes from the system learning your company's specific reporting language and decision frameworks. If it can recognize that a 5% revenue dip in your SaaS business during January is normal but the same dip in March indicates a real problem, then it becomes valuable.\n\nMost automation tools are either too basic for real business intelligence or way too complex for everyday reporting workflows. The sweet spot is AI that understands your business metrics deeply enough to provide contextual analysis instead of just describing charts.\n\nI'd want the system to connect multiple data sources automatically and identify correlation patterns that human analysts miss. Like connecting customer support ticket volume to churn metrics to product release dates without manual analysis.\n\nThe key is making insights specific to your business instead of generic observations that any dashboard could provide.\n3. Score: 1 | Comment: I would ideally love to have a chat-based ai that can answer reliably on the report's data :)\n==============================", "\n            === POST 17 ===\n            Title: Everyone says that we need artificial intelligence, but nobody can explain what it really means for a real data analyst.\n            Score: 44 upvotes\n            Total Comments: 41\n            URL: https://www.reddit.com/r/businessintelligence/comments/1mkujg0/\n            Content: Hey all, have you noticed how \u201cAI\u201d has become some sort of buzzword that everyone throws around? Lot of folks at my job say, \u201cWe should use AI for that,\u201d but when you ask \u201cfor what, exactly?\u201d\u2014the room goes silent. Feels like AI is perceived as a magic fix without anyone really knowing how or why.\n\n\n\nI am curious, What are some real use cases where AI actually helped? And what are those \u201cwe want AI\u201d moments that fell flat? I Would love to hear  your perspective on this?\n\n            TOP COMMENTS:\n        1. Score: 49 | Comment: It significantly, significantly reduces the learning curve and experience required to write any kind code. You still have to have *some* working knowledge, but you no longer need years of SQL experience to write straight forward queries to pull into Power BI for example. \n\nI still write all my own code, been doing it for a long time. But now I'll just write it quick and dirty, copy/paste into ChatGPT, and ask \"clean this up\" or \"make this more efficient\" or \"add one condition that does this\". Done in 15 seconds.\n2. Score: 9 | Comment: If you already handle Robotic Process Automation, then AI is just a more flexible version of this. \n\nAI does really well when you have a defined process, that has post ETL cleaned data, to make other processes in the business run. \n\nIf you treat AI like it's the dumbest fresh college hire, and leverage both coding expertise (object oriented frameworks) and project management, with well defined process documentation. AI can do incredible automations. \n\nFor example I consult with small businesses around their SEO. Because I have a well defined process, and the data is cleaned through an ETL I built, their A/B testing and campaigns now run about \\~10% better without outside intervention and outside the initial setup I spend about 30 mins a week on each account, instead of 4 hours. So I've been able to bring on twice as many clients and work about half as much. The prompt for this is about 3 pages so around 1500 words, I feed it a pdf explaining the SEO strategy, and upper and lower spend limits, as well as the process through the API. It leverages N8N and Claude.\n\nMost individuals are still just putting in prompts and expecting magic, we aren't there yet, but if you break down the process and have AI handle certain tasks, in an automated workflow. It takes a few human elements out, which in my experience so far has produced better results.\n3. Score: 8 | Comment: If the AI can be the equivalent of On-Prem and you feed it all the databases, it can potentially reduce the need for domain-specific analysts, have less of them. \n\nBecause you can ask in plain English for data and the AI will do the SQL code and/or help build the data warehouse. \n\nContext matters. As a DE I see myself using (soon) AI to help document existing workflows by tracking the data from source systems to unified Snowflake/Kimball data. Reverse engineering.\n\nA very simple AI will track all the SQL used in all reports and pipelines, and you ask for a result set in English, and if fed properly, will find an existing report out of hundreds or thousands what is very similar, and extrapolate any missing columns. \n\nThe business analysts would have the training responsibilities. A DE or sysadmin like me will build the LLM virtual machine for secure use within the company.\n==============================", "\n            === POST 18 ===\n            Title: What aspect of your work did you not think would require so much time?\n            Score: 0 upvotes\n            Total Comments: 9\n            URL: https://www.reddit.com/r/businessintelligence/comments/1mk5eoq/\n            Content: I assumed that my days as a BI analyst would be spent delving deeply into data(learning,understanding,etc..) and identifying perceptive patterns. Rather, I've discovered that I'm wasting a large amount of my week just restating dashboards and charts to various executives and stakeholders. To be honest, I'm surprised at how much of my workflow is dominated by this manual translation. Which unforeseen task has grown more significant than you anticipated in your BI role?\n\n            TOP COMMENTS:\n        1. Score: 4 | Comment: Also, Op, the dirty secret of almost every data role is that people really just want something to point at. Or they want something that makes zero sense, like a pie chart with negative values.\n2. Score: 3 | Comment: We're called business intelligence, but we're on the project management office, but we are not coe.\n3. Score: 2 | Comment: ...have you tried inproving the charts?\n==============================", "\n            === POST 19 ===\n            Title: [Throwback Thursday] Exploring open-source alternatives to Confluence\n            Score: 0 upvotes\n            Total Comments: 0\n            URL: https://www.reddit.com/r/businessintelligence/comments/1mjuicm/\n            Content: No text\n\n            TOP COMMENTS:\n        ==============================", "\n            === POST 20 ===\n            Title: Weekly Entering & Transitioning - Thread 11 Aug, 2025 - 18 Aug, 2025\n            Score: 1 upvotes\n            Total Comments: 0\n            URL: https://www.reddit.com/r/datascience/comments/1mn3338/\n            Content:  \n\nWelcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n\n            TOP COMMENTS:\n        ==============================", "\n            === POST 21 ===\n            Title: Catch-22: Learning R through \"hands on\" Projects\n            Score: 3 upvotes\n            Total Comments: 13\n            URL: https://www.reddit.com/r/datascience/comments/1mmzk4s/\n            Content: \n\nI often get told \"learn data science by doing hands-on projects\" and then I get all fired up and motivated to learn, and then I open up R.... And then I stare at a blank screen because I don't know the syntax from memory. \n\nAnd then I tell myself I'm going to learn the syntax so that I can do projects, but then I get caught up creating folders for each function of dplyr and the subfunctions of that and cheat sheets for this.\n\nAnd then I come across the advice that I shouldn't learn syntax for the sake of learning syntax - I should do hands on projects.\n\nI need projects to learn syntax and I need syntax to start doing projects.\n\n            TOP COMMENTS:\n        1. Score: 56 | Comment: You don\u2019t need to memorize the syntax though. Just look it up.\n2. Score: 20 | Comment: You can Google syntax. What you need to do is code. Pseudo code the problem out first and then go step by step looking up what you need to.\n3. Score: 8 | Comment: You do not need syntax to start doing projects, you need a high-level idea with clear outcomes.\n  \nWhat is the problem you are trying to solve? How does solving it have an impact? What are the requirements of the solution? What tools will enable implementation of your solution? How will you assess and interpret the output?\n  \nR, or any other language, will only really be needed for the third question which is a fraction of the total work. If you figure out the what and why, the how (the code) kind of writes itself.\n==============================", "\n            === POST 22 ===\n            Title: Business focused data science\n            Score: 29 upvotes\n            Total Comments: 21\n            URL: https://www.reddit.com/r/datascience/comments/1mly9hm/\n            Content: As a microbiology researcher, I'm far away from the business world. I do more -omics and growth curves and molecular techniques, but I want to move away from biology.\n\nI believe the bridge that can help me do that is data. I have got experience with R and excel. I'm looking at learning SQL and PowerBI.\n\nBut I want to do it away from biology. The problem is, if I was to go from the UK, as a PhD microbiologist, and approach GCC consulting/business analyst recruiters, I get the sense that they'd scoff at me for thinking too highly of my \"transferrable skills\" and tell me that I don't have experience in the world of business.\n\nHow would I get myself job-ready for GCC business-focused data science roles. Is there anyone out there that has made the switch that can share some advice?\n\nThanks in advance\n\n            TOP COMMENTS:\n        1. Score: 33 | Comment: I can tell you as someone who hires data scientists, if someone like you was sat in front of me at interview, I\u2019d be wanting to you to convince me that you\u2019re ready half bake something to meet a deadline, rather than endlessly refine something which will never deliver.\n\nI\u2019ve worked with DSs who made the move and hated it, and some who made the move and never even realised they weren\u2019t able to make (the right kind) of progress.\n2. Score: 9 | Comment: Honestly it\u2019s gonna be super hard in this market. I have a PhD in Math Evolutionary Bio and experience as a data scientist in clinical trials and it\u2019s mostly just reporting, really no ML. Only thing I could get since I can\u2019t get in other industries. I did projects to help me move into more tech focused roles that involved LLMs and data engineering, and really have not had any luck tbh. \n\nThat being said, if you want to be a data analyst, my suggestion is to focus on healthcare insurance companies. Your Microbio experience may be helpful because you maybe handling biological data. You should work on projects using SQL, Python, and Power BI that involve healthcare data, namely handling EHR datasets or just even calculating business metrics in the space. Do your research to get a lay of the land, and then choose a project and show an end to end data pipeline. Extract your data and transform using Python, and load your data into a database to analyze using SQL. Boom. First project.\n3. Score: 3 | Comment: I\u2019m a data scientist at a biotech company and we make money off of growth curves and -omics. \n\nThere are even people in the R&D department just like you that use R. In my department we use python because we have things in production.\n==============================", "\n            === POST 23 ===\n            Title: Burnout, disillusionment, and imposter syndrome after 1 year in DS. Am I just an API monkey? Reality check needed.\n            Score: 80 upvotes\n            Total Comments: 31\n            URL: https://www.reddit.com/r/datascience/comments/1mluc12/\n            Content: Hey folks,\n\nI am about a year into my first data science job. It took roughly a year and more than 400 applications to land it, so the idea of another long search is scary.\n\nEarly on I worked with an internally built causal AI model that captures relationships for further analysis. I did not build the model. I ran experiments to make it more explainable and easier for others to use. I also built data orchestration pipelines using third party tools that are common in industry and cloud providers like AWS and GCP.\n\nThe last six months have shifted to LLM and NLP work. A lot of API calls, large text analysis. The next six months look even more LLM heavy since I am leading an internal tool build.\n\nOn paper there are wins:\n- I have led projects and designed tools from scratch.\n- My communication and client skills have improved.\n\n\nMy concerns:\n\n- I am not doing much classical DS or rigorous modeling.\n- LLM work often feels like API wrangling rather than technical depth.\n- Work life balance i\n\n            TOP COMMENTS:\n        1. Score: 98 | Comment: IMO, life is too short to work on weekends. If you start looking now, it may take you 6 months to land a role anyway. So no harm in starting to brush up on interview prep.\n2. Score: 47 | Comment: Most data science work is software engineering lite and data engineering. Modeling is a tiny sliver, and you often have to scope and sell modeling as something to work on in any company. If LLMs sell, then convince people to work on prompt experimentation frameworks, fine tuning, and representation modeling, for example. The latter is important for interviews and discussing what models you built in prior roles.\n3. Score: 22 | Comment: That\u2019s about where you should be. You won\u2019t get to do the really exciting stuff for another two or three years. You have to put in the hours and build your reputation first.\n\nEdit: new hires and entry levels always want to start doing the really exciting and fun stuff right off the bat. The issue is that such stuff also comes with a huge amount of responsibility and knowing the foundations and grunt work inside and out. It usually takes 3-5 years of grunt work and proving yourself, and realizing what you *don\u2019t* know in order to get to do the stuff you actually trained to do.\n==============================", "\n            === POST 24 ===\n            Title: AI isn't taking your job. Executives are.\n            Score: 1328 upvotes\n            Total Comments: 139\n            URL: https://www.reddit.com/r/datascience/comments/1mlmwk0/\n            Content: If AI is ready to replace developers, why aren't developers replacing themselves with AI and just taking it easy at work?\n\nI'm a Director at my company. I'm in the meetings and helping set up the tools that cost people their jobs. Here's how they work:\n\n1. Claude AI writes some code\n\n2. The code gets passed to a developer for validation\n\n3. Since the developer's \"just validating\", he can be replaced with an overseas contractor that'll work for a fraction of the pay\n\nWe've tracked the tools, and we haven't seen any evidence that having Claude take a crack at the code saves anybody any time - but it does let us justify replacing expensive employees with cheap overseas contractors.\n\nYou're not getting replaced by AI.\n\nYour job's being outsourced overseas.\n\n            TOP COMMENTS:\n        1. Score: 356 | Comment: To validate anything in a larger system, you need to know how it works and why certain implementations were added. You need to know the logical and business context. And you can't learn that quickly. Most of the systems I have worked with lack comprehensive documentation and testing. So replacing a developer with contextual knowledge with someone cheaper will take years.\n2. Score: 90 | Comment: It\u2019s the same playbook as past automation waves. The tool is secondary. The real change is the org deciding they can get 70% of the quality for 30% of the cost and calling it a win.\n3. Score: 61 | Comment: BS, LLM don't write complex code nor analysis. I wish I could use LLm models like you are saying but it does not work like that at all.\n\nPs.: I have a junior working for me who thinks he can vibe code all the way. I asked him to produce a **little** MVP for API. Dude, he made a multiple instance app over engineered at maximum. Something like dozens of files and THOUSANDS lines of code.I deleted everything and wrote a 100 line single app in flask. Yeah, good luck with vibe coding.\n==============================", "\n            === POST 25 ===\n            Title: Just bombed a technical interview. Any advice?\n            Score: 65 upvotes\n            Total Comments: 36\n            URL: https://www.reddit.com/r/datascience/comments/1ml6fxs/\n            Content: I've been looking for a new job because my current employer is re-structuring and I'm just not a big fan of the new org chart or my reporting line. It's not the best market, so I've been struggling to get interviews. \n\nBut I finally got an interview recently. The first round interview was a chat with the hiring manager that went well. Today, I had a technical interview (concept based, not coding) and I really flubbed it. I think I generally/eventually got to what they were asking, but my responses weren't sharp.* It just sort of felt like I studied for the wrong test. \n\nHow do you guys rebound in situations like this? How do you go about practicing/preparing for interviews? And do I acknowledge my poor performance in a thank you follow up email?\n\n*Example (paraphrasing): They built a model that indicated that logging into a system was predictive of some outcome and management wanted to know how they might incorporate that result into their business processes to drive the outcome. I ini\n\n            TOP COMMENTS:\n        1. Score: 47 | Comment: \"causation/correlation, so I talked about controlling for confounding variables and natural experiments.\"\n\n  \nYour post doesn't contain enough information to determine why you think this was wrong?  I mean conducitng natural experiments is one of the ways you try to get causal effects. Switch Back and Synthetic control methods for example are common ways people try to assess this.\n2. Score: 22 | Comment: I bombed a few. It is hard. I feel for you. I bombed one in April of 2022. But I got my current job in June of that year, and I love it. It\u2019s gonna be ok, friend! You can learn and recover from this!\n3. Score: 9 | Comment: The best thing you can do is study up on the questions they asked and prepare for the next one. I have over a decade experience and a few years back I bombed an interview. It happens.\u00a0\n==============================", "\n            === POST 26 ===\n            Title: Resources/tips for someone brand new to model building and deployment in Azure?\n            Score: 19 upvotes\n            Total Comments: 5\n            URL: https://www.reddit.com/r/datascience/comments/1mkzhvp/\n            Content: Context: my current company is VERY (VERY) far behind, technologically. Our data isn't that big and currently resides in SQL Server databases, which I query directly via SSMS.\n\nWhenever a project requires me to build models, my workflow would generally look like:\n\n1. Query the data I need, make features, etc. from SQL Server.\n2. Once I have the data, use Jupyter Notebooks to train/build models. \n3. Use best model to score dataset.\n4. Send dataset/results to stakeholder as a file.\n\nMy company doesn't have a dedicated Dev team (on-shore, at least) nor a DE team. And this workflow works to make ends meet. \n\nNow my company has opened up Azure accounts for me and my manager, but neither one of us have developed anything in it before.\n\nMicrosoft has PLENTY of documentation, but the more I read, the more questions I have, and I feel like my time will be spent reading articles rather than getting anything done.\n\nIt seems like quite a shift from doing everything \"locally\" like what we have been\n\n            TOP COMMENTS:\n        1. Score: 13 | Comment: first tip: ignore 90% of Azure\u2019s menu  \nit\u2019s a labyrinth built for enterprise IT guys, not data folks trying to ship models\n\nyour local \u2192 cloud shift should start small:\n\n1. **Azure ML Studio** \u2014 this is your Jupyter replacement in the cloud spin up a compute instance drop your notebooks there same workflow, now in Azure\n2. **Blob Storage** \u2014 dump raw data here instead of files on your laptop pull from blob, not SSMS (use Data Factory or just python SDK to move files if needed)\n3. **Azure SQL** \u2014 mirror the prod SQL server if possible keeps you from breaking live stuff while testing\n4. **ML Pipelines** \u2014 once you\u2019ve got a model that works, wrap it in a pipeline to retrain or batch score you\u2019re not deploying to the world yet you\u2019re just creating repeatable steps\n\nskip reading endless docs  \nfind one end-to-end tutorial with *your stack* (SQL \u2192 notebook \u2192 model \u2192 endpoint)  \ncopy it  \nthen tweak it for your use case\n\n[NoFluffWisdom Newsletter](https://NoFluffWisdom.com/Subscribe) has some clean, no-bloat strategies on transitioning messy local workflows into clean cloud pipelines worth a peek\n2. Score: 5 | Comment: Checkout ProjectPro project templates.. you may not want to subscribe but you can refer to them for ideas at least. Look at how their projects use different Azure services to solve real world problems and see which one aligns with you. Explore the details on their page further.\n3. Score: 1 | Comment: I feel very seen by this post haha\n==============================", "\n            === POST 27 ===\n            Title: \"SemiAuto\" Fully Automated Machine Learning Lifecycle by Just API Calling\n            Score: 0 upvotes\n            Total Comments: 6\n            URL: https://www.reddit.com/r/datascience/comments/1mkov0u/\n            Content: So for the last 4 months I have been working on this project which was first supposed to be a upgrade of AutoML, but I later recognised it's potential.\n\nThis project could be one of the best things in ML reasearch, This project is just that good.\n\nFor context, I have the knowledge around ML for about 1.5 years now and thanks to the tools available, I have been able to build a grand project like this,\n\nThe Project's or you can say the Tool name is 'SemiAuto', A full fledged ML lifecycle Automation tool. It has 3 microservice, Regression, Classification, and Clustering.\n\nI have completely build the Version 1 of this project.\n\n\nIt has 6 parts, First ingest the Data.csv file and the target column.\n\nSecond choose whatever preprocessing you want to and apply them.\n\nThird use feature tools to build new features and then SHAP to select the amount of features you want.\n\nFourth choose any algorithm you want with the hyper params and build the model.\n\nFifth choose the optimization technique and g\n\n            TOP COMMENTS:\n        1. Score: 5 | Comment: Don't really want to dehype you on your project which you're passionate about, but a slight reality check.\n\nFirst, this likely won't be used for research. Research needs a high degree of customization, while this looks like a low-code (i.e. functionally limiting) wrapper aimed for non-ML product people or juniors.\n\nSecond, you should clearly document what exactly your tool is/will be capable of. Which input formats it can work with, what preprocessing steps and models are available, what kind of model optimization can be done, etc.\n\nThird, you didn't mention how do you interact with it. Is it a pypi package, or a web app, or do you run it in CLI, or something else? Open sourcing it would be a plus too.\n\nLastly, you say it's mind blowing and the best thing ever. That's a bold claim without solid evidence behind it, this might hurt your credibility. Also such hype combined with little technical detail suggests you maybe still be early in your ML career. Maybe try to gain more work experience first to better understand which parts of ML pipelines are the most problematic. Irl projects differ significantly from uni/online course practice projects.\n\nAnyway, don't be discouraged, this looks like a solid project, likely will attract attention of many hiring managers.\n2. Score: 2 | Comment: Hi, congratulations on this project. Please provide the link to try out.\n3. Score: 1 | Comment: Well, if you already have a data.csv with tabular data and a target you have done 99% of the work of a professional data scientist :D \n\nSo nice work, it's good you have learned a few things but I don't really see a useful application for this.\n==============================", "\n            === POST 28 ===\n            Title: How would you visualize or analyze movements across a categorical grid over time?\n            Score: 14 upvotes\n            Total Comments: 8\n            URL: https://www.reddit.com/r/datascience/comments/1mkmjje/\n            Content: I\u2019m working with a dataset where each entity is assigned to one of N categories that form a NxN grid. Over time, entities move between positions (e.g., from \u201cN1\u201d to \u201cN2\u201d).\n\nHas anyone tackled this kind of problem before? I\u2019m curious how you\u2019ve visualized or even clustered trajectory types when working with time-series data on a discrete 2D space.\n\n            TOP COMMENTS:\n        1. Score: 14 | Comment: Is the probability of transition from one category to another time invariant?\n\n\nIf so, a discrete time Markov chain approach would be suitable.\n2. Score: 6 | Comment: If the time component is discrete then a Sankey is an option depending on your data and the end goal.\n3. Score: 3 | Comment: Do you think this is a graph problem?\n==============================", "\n            === POST 29 ===\n            Title: How do you analyse unbalanced data you get in A/B testing?\n            Score: 28 upvotes\n            Total Comments: 26\n            URL: https://www.reddit.com/r/datascience/comments/1mkdy7a/\n            Content: Hi \nI have two questions related unbalanced data in A/B testing. Would appreciate resources or thoughts. \n\n1. Usually when we perform A/B testing, we have 5-10% in treatment, after doing power analysis we get the sample size needed, we run tge experiment, by the time we get required sample size for treatment we get way more control samples, so now when we analyse, which samples do we keep in control group? For example by the time we collect 10k samples from treatment we might get 100k samples of control. So what to do now before performing t-test or any kinds of test? \n (In ML we can downsample or over sample but what to do in causal side) \n\n2. Again similar question Lets say we are performing test on 50/50 but if one variant get way more samples as more ppl come through that channel and common for users, hiw do we segment users such as way? And again which samples we keep once we get way more sample than needed? \n\nI want to know how it is tackeled in day to day, and this thing happen \n\n            TOP COMMENTS:\n        1. Score: 15 | Comment: There\u2019s no need to downsample your control group. You get a bit more power with the larger group and you can still get all the same metrics for your significance testing. For sample size calculations, there are ways of adjusting for imbalanced sets, but you can simplify by using the smaller set as your sample size for all variants. \n\nIf you are getting more people in treatment than control when expecting 50/50, you have a sampling bias. Make sure you aren\u2019t using business rules to approximate a balanced sample; users should be randomized. Ie if you have some users entering your app via paid marketing and half via organic, half of those marketing-attributable users should be in treatment and half of the organic users should be in treatment.\n2. Score: 7 | Comment: You need to assign to control and treatment randomly. It seems you are only selecting into the treatment random units from the population, and the rest is in the control? That's not how it works.\n3. Score: 1 | Comment: Isn\u2019t there a paper by Netflix\u2019s data science team on solving a similar problem, but with sampling over time and repeated p-testing?\n=============================="]